{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7e8058b",
   "metadata": {},
   "source": [
    "# Purpose of Machine Learning\n",
    "ML is a great tool to analyze data, find hidden patterns and rlts,and extract info to enable info-driven decisions and provide insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1905414a",
   "metadata": {},
   "source": [
    "## ML Terminologies\n",
    "Observations----------Records, Samples, Examples that may contain one or more data points\n",
    "\n",
    "Features---------- Inputs or Attributes that define a given data set and are columns in a spread sheet\n",
    "\n",
    "Response--------Label, Outcome, target or some defined answers attached to the data set for the given set of data points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f8bf14",
   "metadata": {},
   "source": [
    "## ML Approach\n",
    "Starts with either a problem that uneed to solve or a given dataset that u need to analyze\n",
    "\n",
    "======Understand the problem/dataset\n",
    "======Extract the features from the set\n",
    "======Identify the problem type ie whether it is categorical or cont\n",
    "======Choose the right model\n",
    "======Train and test the model\n",
    "======Strive for accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe73262",
   "metadata": {},
   "source": [
    "ML can either be supervised or unsupervised. The problem type should be selected based on the type of learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f79b2",
   "metadata": {},
   "source": [
    "### Concept\n",
    "#### 1. Supervised Learning\n",
    "---the dataset used to train a model should have observations,features and responses. The model is trained to predict the 'right' response for a given set of data points\n",
    "\n",
    "---models are used to predict an outcome\n",
    "\n",
    "---the goal of this model is to 'generalize' a set so that the 'general rule' can be applied to a new data as well\n",
    "\n",
    "\n",
    "##### Data  Type\n",
    "Continous or Categorical\n",
    "###### Problem Type\n",
    "Regression or Classification\n",
    "\n",
    "####### Categories of nes based on the topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb41296",
   "metadata": {},
   "source": [
    "#### 2. Unsupervised Learning\n",
    "---ther response or the outcome of the data is not known\n",
    "\n",
    "---models re used to identify and visualize patterns in data by grouping similar types of data\n",
    "\n",
    "---the goal of the model is to 'represent' data in a way that meaningful info can be extracted\n",
    "\n",
    "#### Data  Type\n",
    "Continous or Categorical\n",
    "###### Problem Type\n",
    "Dimensionality reduction or Clustering\n",
    "\n",
    "####### Example Grouping of similar stories on differentnews network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630b15f6",
   "metadata": {},
   "source": [
    "##### How it works Supervised\n",
    "\n",
    "Aknow dataset with observations, features, and response is used to create and train a ML algorithm. a predictive model, built on top of this algorithm , is the used to predict the response for a new set that has te same features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f9bca0",
   "metadata": {},
   "source": [
    "##### How it works Unsupervised\n",
    "a know dataset has a set of observations with features. but the response is not known the predictive model uses this features to identify how to classify and represent the data points of new or unseen data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b1c068c",
   "metadata": {},
   "source": [
    "To train Supervised learning models, data analysts usually divide  a known dataset into training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ed408a",
   "metadata": {},
   "source": [
    "### Scikit Learn\n",
    "a powerfull and modern ml python lib for fully and semi automated data analysis and info extraction. it has:\n",
    "\n",
    "==== efficient tools to identify and organize problems(sup/unsup)\n",
    "==== free and open datasets\n",
    "==== rich set of libs for learning and predicting\n",
    "==== model support for every problem type\n",
    "==== model persistence\n",
    "==== open source community and vendor support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40503a47",
   "metadata": {},
   "source": [
    "###  Sklearn - Problem-Solution Approach\n",
    "it helps data scientists organize their work through its problems-soln approach by:\n",
    "== model selection & alg based on the data set type\n",
    "== estimator object reprs the model in sklean \n",
    "== model training feeding the data to model\n",
    "== predictions forcast new sets\n",
    "== model tuning thro multipleiterations\n",
    "== accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83fecee",
   "metadata": {},
   "source": [
    "### Sklearn - Problem-Solution Considerations\n",
    "- Create separate objects for feature and response\n",
    "- ensure that features and response have only numericvalues\n",
    "- features and response should be inthe formof numpy nd array\n",
    "- since features and response should be inthe form arrays they should have shapes and sizes\n",
    "- features are always mapped as xand respnse as y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca42c25e",
   "metadata": {},
   "source": [
    "## Supervised Learning Models : \n",
    "### 1. Linear Regression\n",
    "- used to analyse continous data\n",
    "- most basic and widely used technique to predict a value of an attribute\n",
    "- pretty easy to use as the model doesnt require a lot of tuning\n",
    "- itrunsvery first thus time efficient"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a269fc7a",
   "metadata": {},
   "source": [
    "the linear regression eqn is based on the formula for a simple linear eqn:\n",
    "y = mx + c\n",
    "where:\n",
    "- c is the intercept\n",
    "- x is the input feature\n",
    "- m is the coeff of x\n",
    "- y is the response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef253214",
   "metadata": {},
   "source": [
    "the smaller the value of SSR/SSE the more accurate the predictions which would make the model the best fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e900b00f",
   "metadata": {},
   "source": [
    "How does LR work?\n",
    "sklearn(class).linear_model.LR(fit_intercept(calculates he intercept for the module)=True,normalize=False(normalize the regression variable b4 performing the reg operation),copy_X=True(copies the regvar),n_jobs=1(no. of jobs tobe used for the computation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11c13b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57906e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelida\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#import the boston dataset from the sklearn lib\n",
    "from sklearn.datasets import load_boston\n",
    "boston_df = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d0b599f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use the built in method DESCR to explore and understand the data\n",
    "print(boston_df['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38de1126",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "#print the features of the set\n",
    "print(boston_df['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ad34ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the data ina df\n",
    "df_boston = pd.DataFrame(boston_df.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c8f8d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set features as columns in the df\n",
    "df_boston.columns = boston_df.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8b6a031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view the first five obs\n",
    "df_boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1133b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n"
     ]
    }
   ],
   "source": [
    "#print the dataset matrix(obs and feat matrix)\n",
    "print(boston_df.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3817ff59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "#print the shape of the target or response col\n",
    "print(boston_df.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32da50cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4\n",
      " 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8\n",
      " 18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6\n",
      " 25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4\n",
      " 24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9\n",
      " 24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9\n",
      " 23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7\n",
      " 43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8\n",
      " 18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4\n",
      " 15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8\n",
      " 14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4\n",
      " 17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8\n",
      " 23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2\n",
      " 37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.\n",
      " 33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.\n",
      " 21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1\n",
      " 44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5\n",
      " 23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8\n",
      " 29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8\n",
      " 30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1\n",
      " 45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9\n",
      " 21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2\n",
      " 22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1\n",
      " 20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1\n",
      " 19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6\n",
      " 22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8\n",
      " 21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3\n",
      " 13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2\n",
      "  9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.\n",
      " 11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4\n",
      " 16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3\n",
      " 11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6\n",
      " 14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7\n",
      " 19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3\n",
      " 16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.\n",
      "  8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9\n",
      " 22.  11.9]\n"
     ]
    }
   ],
   "source": [
    "#view the data within the target or respons column\n",
    "print(boston_df.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480141d0",
   "metadata": {},
   "source": [
    "#### Demonstrate how to create and train a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc678094",
   "metadata": {},
   "outputs": [],
   "source": [
    "#append price the target var as a new column to the df_boston\n",
    "df_boston['Price'] = boston_df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c94ae910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  Price  \n",
       "0     15.3  396.90   4.98   24.0  \n",
       "1     17.8  396.90   9.14   21.6  \n",
       "2     17.8  392.83   4.03   34.7  \n",
       "3     18.7  394.63   2.94   33.4  \n",
       "4     18.7  396.90   5.33   36.2  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the top five observations\n",
    "df_boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "691e6b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign features on the x axis\n",
    "x_features = boston_df.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8fe2830",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign target on y axis\n",
    "y_target = boston_df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3ace1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import linear model which is the setimator/model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#instatniate it\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4ce7b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the data into the estimator\n",
    "lr.fit(x_features, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b9aa342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimated intercept is: 36.46\n"
     ]
    }
   ],
   "source": [
    "#print the intercept\n",
    "print('The estimated intercept is: %.2f' % lr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d04101e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The coeffiient is: 13\n"
     ]
    }
   ],
   "source": [
    "#print the coeff\n",
    "print('The coeffiient is: %d' % len(lr.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aefef305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model by splitting the whole df into train and test dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_features, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4bca755a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the shape of the dataset\n",
    "boston_df.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2d8ba26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((379, 13), (127, 13), (379,), (127,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the shapes of the training and testing datasets\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d39c3b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the training set into the model\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "141463e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 23.877399\n"
     ]
    }
   ],
   "source": [
    "#calculate the mse or rss\n",
    "print('MSE: %2f' % np.mean((lr.predict(X_test) - y_test)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0d958f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance score: 0.636884\n"
     ]
    }
   ],
   "source": [
    "#calculate the variance scor\n",
    "#NB the closer the variance is to one the better the model\n",
    "print('Variance score: %2f' % lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864658d0",
   "metadata": {},
   "source": [
    "### 2. Logistic Regression\n",
    "A generalization of the linear regression model used for classification problems\n",
    "\n",
    "#### class sklearn.linear_model.LogisticRegression(class selecting the estimator/object/model)\n",
    "(penalty='l2(specifies the norm used in penalization),  dual=False(implemented only for l2 penalty), tol=0.0001, C=1.0(inverse of regularization), fit_intercept=True(calculates the intercept), intercept_scaling=1, class_wight=None, random_state=None(seed or the randome state instance), solver='liblinear'(alg to use inthe optimization prblem), max_iter=100, multi_class='ovr'(can be ovr(binary) or multinomial), verbose=0, warm_start=False(if true reuse the soln of the previous call), n_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890eeada",
   "metadata": {},
   "source": [
    "### 3. K Nearest Neighbors (K-NN)\n",
    "knn is one of the simplest ml alg used for both classffication and regression problem types\n",
    "- it uses the entire training dataset to create its model\n",
    "- it looks at the inputs or features of the training dataset to identify the attributes or any new unseen data\n",
    "- based on how near it is to the input data point, the algorithm classifies it.\n",
    "- if using this method for binary classification, chooce an odd no.for k to avoid the case of a 'tied distance between two claases'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ec96ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sklearn load data set\n",
    "from sklearn.datasets import load_iris\n",
    "iris_dataset = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "353e83c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#display the dataset typy\n",
    "type(iris_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c358f046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "#view info using data ste built in method descr\n",
    "print(iris_dataset['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70db9d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "#view features\n",
    "print(iris_dataset.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80c8a07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "#view target response\n",
    "print(iris_dataset.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6443b84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "#find number of observations\n",
    "print(iris_dataset.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "71fa60a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign features data to x-axis\n",
    "x_feature = iris_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "935751ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign target data to y axis\n",
    "y_target = iris_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "618b83af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "#view the shape for both axis\n",
    "print(x_feature.shape, y_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b91717e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first use knn classifier method. import it from sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae1af2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate the knn estimator\n",
    "# n_nb as one so that the estimator will look for the first nearest nb\n",
    "knn = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d4361a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_neighbors=1)\n"
     ]
    }
   ],
   "source": [
    "#print the knn\n",
    "print(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c30bb6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit data into knn model(estimator)\n",
    "knn.fit(x_feature, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "65a8520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create objects with new features for predictions\n",
    "x_new = [[3,5,4,1],[5,3,4,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0c0f071e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict he outcome for the new observations using knn classifier\n",
    "knn.predict(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "604d242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the logistic regression estimator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "54f77606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelida\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit data into the logistic regr estimator\n",
    "logr.fit(x_feature, y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a666890d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict the outcome using the logistic regression model \n",
    "logr.predict(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c1dd90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa6d1ecd",
   "metadata": {},
   "source": [
    "## Unsupervised Learning Models :\n",
    "### 1. Clustering\n",
    "A cluster is a group of similar data points and is uset to :\n",
    "- extract the structure of te data\n",
    "- identify groups in the data\n",
    "\n",
    "\n",
    "-- NB Greater similarity between data points results in better clustering"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cfa78fdc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6a5d90f",
   "metadata": {},
   "source": [
    "###  2. K-Means Clustering\n",
    "K-means finds the best centroids by alternatively assigning random centroids to a data set and selecting mean data points from the resulting clusers to form new centroids. it continues this process iteratively until the model is optimized\n",
    "- assigns random *k* clusters to a set\n",
    "- assigns data points to the centroids depending on their proximity to them\n",
    "- choosea mean from each clusteras a centroid\n",
    "- reassigns datapoints to these new centroids based on proximity \n",
    "- iterates the process till the model is optimized"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e072cfcf",
   "metadata": {},
   "source": [
    "##### sklearn.clusster.Kmeans(class)\n",
    "(n_clusters=8(number of clusters to form & no.of centroids to generate), \n",
    "    init='k-means++'(select initial cluster centers),\n",
    "    n_init=10(no. of times the kmeansalg will be run with diff centroid seeds),\n",
    "    max_iter=300(max no. of iter of the kmeans alg for a single run),\n",
    "    tol=0.0001, precompute_distances='auto'(pre-compute for faster operation),\n",
    "    verbose=0, random_state=None(initialize the centers),\n",
    "    copy_x=True(if true does not modify data while pre-compting),\n",
    "    n_jobs=1(no. of jobs in parallel computation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "29cf43a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import kmeans cluster for sklearn.cluster\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#import make bobs data set fromthe sklearn cluster\n",
    "from sklearn.datasets import  make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "55e46506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 0, 2, 2, 1, 1, 1, 1, 0, 1, 1, 2, 2, 0, 0, 0, 2, 1, 0, 2,\n",
       "       0, 1, 1, 1, 1, 0, 0, 2, 2, 0, 2, 2, 1, 0, 1, 1, 0, 2, 1, 2, 2, 1,\n",
       "       0, 2, 0, 2, 2, 1, 2, 0, 0, 1, 0, 2, 1, 2, 1, 1, 2, 0, 0, 0, 2, 2,\n",
       "       0, 0, 1, 0, 1, 2, 2, 1, 2, 1, 1, 1, 0, 0, 2, 1, 2, 2, 0, 2, 1, 0,\n",
       "       0, 2, 1, 2, 2, 0, 0, 2, 1, 2, 1, 0, 2, 2, 0, 1, 1, 2, 0, 0, 2, 2,\n",
       "       0, 0, 1, 0, 2, 1, 0, 2, 2, 1, 1, 0, 1, 0, 0, 1, 2, 0, 1, 1, 2, 2,\n",
       "       0, 0, 1, 1, 2, 2, 2, 0, 2, 0, 2, 2, 1, 0, 1, 1, 0, 2, 1, 1, 2, 2,\n",
       "       0, 1, 1, 2, 0, 2, 2, 1, 1, 1, 0, 2, 0, 2, 2, 0, 2, 0, 1, 0, 0, 2,\n",
       "       1, 2, 0, 0, 1, 1, 2, 1, 2, 1, 0, 0, 0, 1, 2, 2, 0, 1, 0, 0, 0, 1,\n",
       "       1, 2, 0, 2, 0, 1, 2, 1, 0, 2, 0, 2, 0, 1, 2, 0, 2, 2, 2, 0, 2, 0,\n",
       "       0, 1, 1, 2, 0, 2, 1, 1, 2, 1, 0, 0, 1, 2, 0, 2, 2, 1, 1, 1, 0, 0,\n",
       "       2, 0, 0, 1, 0, 1, 2, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 2, 1, 2,\n",
       "       1, 1, 2, 1, 2, 2, 0, 0, 1, 0, 1, 0, 2, 0, 2, 1, 2, 0, 1, 0, 1, 2,\n",
       "       2, 2, 1, 2, 0, 0, 1, 1, 2, 1, 0, 0, 1, 2])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define no. of samples\n",
    "n_samples = 300\n",
    "\n",
    "#define random state value to iniciate thecenter\n",
    "random_state = 20\n",
    "\n",
    "#define no.of features as 5\n",
    "X,y = make_blobs(n_samples=n_samples, n_features=5, random_state=None)\n",
    "\n",
    "#define no. of clusters to be formed as 3 and in random state & fit features into the model\n",
    "predict_y = KMeans(n_clusters=3, random_state=random_state).fit_predict(X)\n",
    "\n",
    "#print the estimator prediction\n",
    "predict_y                                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e10ffcb",
   "metadata": {},
   "source": [
    "### 3. Dimensionality Reduction\n",
    "It reduces a high dim dataset into a data set with fewer dimensions. This makes it easeir and faster for the algorthim to analyse data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
