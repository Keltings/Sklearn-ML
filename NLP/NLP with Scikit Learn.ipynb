{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd28e5ec",
   "metadata": {},
   "source": [
    "# NLP  or Natural Language Processing\n",
    "an automated way to understand and analyze natural human language and extract informaion from such data by applying machine algorithms\n",
    "- ie extract the linguistic information \n",
    "- the content could be image, audio, video or text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb560c9",
   "metadata": {},
   "source": [
    "### Why NLP\n",
    "The world is now connected globally due to the advancement of technology and devices this has resulted in the high volume of digital data across the world and has led to a number of challenges in analyzing data including:\n",
    "- analyzing tones of data generated in the form of text, image, audios and videos\n",
    "- identifying various languages and dialects followed across the globe\n",
    "- applying quantitative analysis on huge collections of data\n",
    "- handling ambiquities while interpreting data and extracting information\n",
    "\n",
    "#### With this advancement in tech and services the world is now a global village and this is wherenlp proves useful\n",
    "\n",
    "##### One of the main goals of NLP :\n",
    "- Understand various languages, process them and extract information from them.\n",
    "\n",
    "\n",
    "In NLP full automation can be easily achieved by using modern software libraries, modules, and packages\n",
    "With the help of these libraries we can also build analytical models and automate the nlp process with minimum or no human interventions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac220b",
   "metadata": {},
   "source": [
    "### NLP Terminology\n",
    "###### 1. Word boundaries\n",
    "- determines where one word ends and another begins\n",
    "###### 2. Tokenization\n",
    "- a technique to split words, phrases, idioms etc,present in a doc\n",
    "###### 3. Stemming\n",
    "- a process to map wordsto thire stem or root. It is very useful in finding synonyms, and extensiviley used in search engines\n",
    "###### 4. Tf-idf\n",
    "- a numerical value which represents how important a word is to a document or corpus\n",
    "###### 5. Semantic analytics\n",
    "- a technique in vectorial semantics of analysing relationships between a set of docs and the terms it contains by producing a set of concepts related to the documents and terms\n",
    "i.e compares words phrases and idioms in a set of documents to extract meaning\n",
    "###### 6. Disambiguation\n",
    "- Technique to determine the meaning and a sense of words(context vs intent)\n",
    "###### 7. Topic models\n",
    "- type of statistical model that find abstract topics which occur in a collection of documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d011ef",
   "metadata": {},
   "source": [
    "### NLP Approach for Text Data\n",
    "This approaches can be interalated or independently applied depending on the type of data to analyze\n",
    "- basic text processing - extract key words\n",
    "- categorizing and tagging words - eg tag a word using chines\n",
    "- classify text - identify particular feature of language\n",
    "- extract information - identify relationships in a structured way\n",
    "- analyze sentence structure - capture formal gramma\n",
    "- build feature based structure - insight into text arguments\n",
    "- analyze the meaning - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d4af4b",
   "metadata": {},
   "source": [
    "##### Eliminate punctuations and stopwords from the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1a02265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required library\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ba3de2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view first 10 stopwords present in the english corpus\n",
    "stopwords.words('english')[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9cb78a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a test senetnce\n",
    "test = 'This is my first test string. Wow!! we are doing just fine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "995aed60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'm',\n",
       " 'y',\n",
       " ' ',\n",
       " 'f',\n",
       " 'i',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 't',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 's',\n",
       " 't',\n",
       " 'r',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'W',\n",
       " 'o',\n",
       " 'w',\n",
       " ' ',\n",
       " 'w',\n",
       " 'e',\n",
       " ' ',\n",
       " 'a',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'd',\n",
       " 'o',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'j',\n",
       " 'u',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 'f',\n",
       " 'i',\n",
       " 'n',\n",
       " 'e']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# noweliminate the punctuations and print them as a whole sentence\n",
    "no_punctuation = [char for char in test if char not in string.punctuation]\n",
    "no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d4e1a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is my first test string Wow we are doing just fine'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_punctuation = ''.join(no_punctuation)\n",
    "no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0dad540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'my',\n",
       " 'first',\n",
       " 'test',\n",
       " 'string',\n",
       " 'Wow',\n",
       " 'we',\n",
       " 'are',\n",
       " 'doing',\n",
       " 'just',\n",
       " 'fine']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split each word present in the new sentence\n",
    "no_punctuation.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c4df82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now eliminate stopwords\n",
    "clean_sentence = [word for word in no_punctuation.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2292e2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first', 'test', 'string', 'Wow', 'fine']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the final cleaned sentence\n",
    "clean_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627e8558",
   "metadata": {},
   "source": [
    "### NLP Application\n",
    "#### 1. Machine Translation\n",
    "    - Machine translation is used to translate one language into another. Google translate is an example since it uses NLP to translate the input data from one lanuage to another\n",
    "    \n",
    "#### 2. Speech Recognition\n",
    "    - Speech recognition app understands human speech and uses it as input information. It is useful for apps eg Siri, Google Now and Microsoft Cortana\n",
    "\n",
    "#### 3. Sentiment Analysis\n",
    "    - Achieved by processing tons of data received from different interfaces and sources. eg nlp uses all social media activities to find out the popular topics of discussions or importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dafbc79",
   "metadata": {},
   "source": [
    "### Major NLP Libraries\n",
    " - NLTK\n",
    "\n",
    "Widely used in the industry to build programs to work with different human lanuages\n",
    " - Scikit-learn\n",
    "\n",
    "Powerful open source package designed for operating with other python libraries eg numpy and pandas\n",
    " - TextBlob\n",
    "\n",
    "Used for processing text data and provides a simple API'S for diving into NLP\n",
    " - spaCy\n",
    "\n",
    "A librarythat provides multiple useful views of textual meaning and lingusitic structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e194b7e1",
   "metadata": {},
   "source": [
    "### The Scikit-Learn Approach\n",
    "A very poerful library with set of modules to process and analyse natural language data such as texts, images and extract information using ML algorithms\n",
    " - Built in module to load the datasets, content and categories\n",
    " \n",
    " - Feature extraction, a way to extract information from data that can be text or images\n",
    " \n",
    " - Model training, analyze the content based on particular categories and then train them according to a specific model\n",
    " \n",
    " - Pipeline building mechanism, a technique in sklearn to streamline the NLP process into stages\n",
    "      -  Vectorization - coverting a collection of text of documents into a numerical feature vector\n",
    "      -  Transformation - extract features around the word of interest\n",
    "      -  Modeltraining and application - required for accurate predictions\n",
    " \n",
    " - Performance Optimization, we train the model to optimize the overall process\n",
    " \n",
    " - Grid search for finding good parameters, a powerfull way to search parameters affecting the outcome for model training purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca0897e",
   "metadata": {},
   "source": [
    "#### Modules to Load Conent and Categories\n",
    "sklearn has many built in datasets. There are several methods to load thes datasets with the help of a data load object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4d79350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data set\n",
    "#load_data = sklearn.datasets.load_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f49029",
   "metadata": {},
   "source": [
    "Text files are loaded with categories as subfolder names\n",
    "\n",
    "The folder names are used as supervised signal label names\n",
    "\n",
    "It doesn't try to extract features into a numpy array or scipy sparse matrix\n",
    "\n",
    "If load conetnt parameter is false, it does not try to load the file in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0bd6d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a feature extraction transformer\n",
    "#from sklearn.feature_extraction.text import <appropriate transformer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54ad4fe",
   "metadata": {},
   "source": [
    "##### Attributesof a data load objects are :\n",
    " - Bunch, contains fields and can be accessed as dict keys or an object\n",
    " - Target names, list of requested categories\n",
    " - Data, attribute in the memory where files are loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc279909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "160fa3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create object of the load daa\n",
    "digit_data = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1994f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 1797\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use  built in descr fn to describe the data set\n",
    "digit_data.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdad456c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view type of data\n",
    "type(digit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "139a0fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "       [ 0.,  0., 10., ..., 12.,  1.,  0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view data\n",
    "digit_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cea506c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 8])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view target\n",
    "digit_data.target\n",
    "#the arrays displayed are the response data which is present in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56271c04",
   "metadata": {},
   "source": [
    "#### Feature Extraction\n",
    "a method to convert the conent intonumerical vectors to perform ml\n",
    " 1. Text feature extraction eg large datasets or documents\n",
    " \n",
    " 2. Image feature extraction eg patch extraction/connectivity graph of an image and hierachical clusering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b275f1",
   "metadata": {},
   "source": [
    "#### Bag of Words\n",
    "used to convert text data int numerical feature vector witha fixed size\n",
    "- tokenize - count number of occurences of each word - store as the value feature"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b850e747",
   "metadata": {},
   "source": [
    "class\n",
    "sklearn.feature_extraction.text.CountVectorizer- specifies the number of components to keep\n",
    "(input = 'content',-file name of sequence of string\n",
    "encoding='utf-8', - encoding used to decode the input\n",
    "decode_error='strict',\n",
    "strip_accent=None,-removes accents\n",
    "lowercase=True,\n",
    "preprocessor=None,\n",
    "tokenizer=None,-overides string tokenizer\n",
    "stop_words=None, - built in stopwords list\n",
    "token_pattern='(?u)\\b\\w\\w+\\b',\n",
    "ngram_range=(1,1),\n",
    "analyzer='word',\n",
    "max_df=1.0, max threshhold which should be ignored\n",
    "min_df=1, - min threshhold which should be ignored\n",
    "max_features=None,\n",
    "vocubulary=None,\n",
    "binary=False,\n",
    "dtype=<class'numpy.int64'>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48bc2152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56936796",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate the vectorizer\n",
    "vector = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6bda5abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 3 documents\n",
    "doc1 = 'Hi How are you'\n",
    "doc2 = 'today is a very very very pleasant day we can have some fun fun fun'\n",
    "doc3 = 'this was an amazing experience'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70147ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put them together\n",
    "docs = [doc1, doc2, doc3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf8563ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit them as bag of words\n",
    "bag_of_words = vector.fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f732c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check bag of words\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d709987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply tranfrom method\n",
    "bag_of_words= vector.transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f2aa6927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 18)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 6)\t3\n",
      "  (1, 7)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 15)\t3\n",
      "  (1, 17)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 13)\t1\n",
      "  (2, 16)\t1\n"
     ]
    }
   ],
   "source": [
    "#print bag_of_words\n",
    "print(bag_of_words)\n",
    "# first no is the tuple and teh second is the frequency of words\n",
    "#the tuplehere indicates the doc no  and feature indices of each word which belongs to the document\n",
    "#feature inices are genertaed from the transformmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e724dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#verify the vocubulary for repeated words\n",
    "print(vector.vocabulary_.get('very'))\n",
    "print(vector.vocabulary_.get('fun'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fcb344c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the type of bag_of words\n",
    "type(bag_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b86c749",
   "metadata": {},
   "source": [
    "### Text Feature Extraction Consideration\n",
    "- Sparse\n",
    " -- Utility to use sparse matrix while storing them in memory\n",
    "- Vectorizer\n",
    " -- Implements tokenization and occurence\n",
    "- Tf-idf\n",
    " -- Term weighing utility for term frequency and inerse document frequency\n",
    "- Decoding\n",
    " -- This utlity can decode text files if their encoding is specified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d94e333",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "An important task in model training is to identify the right model for the given set. The choice of model completely depends on the type of the set.\n",
    "\n",
    "#### a) Supervised\n",
    "Models predict the outcome of new observtions and datasets, and classify docs based on the features and response of a given dataset\n",
    " - Naive Bayes, SVM, linear regression, K-NN neighbors\n",
    " \n",
    "#### b) Unsupervised\n",
    "Models identify patterns in the data and extrct its structures. They are also used to group docs using clustering algorithms\n",
    " - K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d90b69",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classifier\n",
    "Most basi technique for classification of text\n",
    "\n",
    "##### Advantages: \n",
    "    - Efficient as it uses limited CPUand memory\n",
    "    - Fast as the model training takes less time\n",
    "  \n",
    "##### Uses:\n",
    "    - Sentiment analysis, email spam detection, categorization of docs and languages detections\n",
    "    - Multinomial Naive Bayes is used when multiple occurrences of the word matter"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b0f729e",
   "metadata": {},
   "source": [
    "class sklearn.naive_bayes.MultinomialNB(\n",
    "alpha=1.0,--smooth parameter(0 for no smoothing)\n",
    "fit_prior=True,--learn class prior probabilities\n",
    "class_prior=None -- prior probabilities of the class\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd6d64f",
   "metadata": {},
   "source": [
    "#### Grid search and Multple Parameters\n",
    "Doc classifiers can have many parameters and a grid approach helps to search the best parameters for model training and predicting the outcome acurately\n",
    "The whole dataset can be divided into multple grids and a search can be runon multiple grids or a combination of grids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9079a52e",
   "metadata": {},
   "source": [
    "#### Pipeline\n",
    "A combination of vectorizers, transformers and model training.\n",
    "###### 1. Vectorizers--converting a collection of text dcs into a numerical feature vector\n",
    "###### 2. Transformer(Tf-idf)--extracts features around the word of interest\n",
    "###### 3. Model Training(doc classifier)--helps the model predic accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad2360a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required libraries\n",
    "import pandas as pd\n",
    "import string\n",
    "from pprint import pprint\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "03fba866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the dataset\n",
    "spam_df = pd.read_csv('SpamCollection.csv', sep='\\t',names=['response','message'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1acc8a",
   "metadata": {},
   "source": [
    "The \\t indicates the data in the spam data set is tab separated or tab delimeted and SCPfunction takes it as a parameter.\n",
    "Message is the text present in the data set wich is a feature and response is the label or category of the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "76114d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  response                                            message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view first five records with head method\n",
    "spam_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be6b57d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import text processing libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#import SGD classifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#import for grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#import for pipe line\n",
    "from sklearn.pipeline import Pipeline\n",
    "#define the pipeline\n",
    "pipeline = Pipeline([\n",
    "            ('vect', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', SGDClassifier())]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1a404343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for grid search\n",
    "parameters = {'tfidf_use_idf':(True, False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bbb3b04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search now...\n",
      "Parameters: \n",
      "{'tfidf_use_idf': (True, False)}\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter tfidf_use_idf for estimator Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n                ('clf', SGDClassifier())]). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\kelida\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n    r = call_item()\n  File \"C:\\Users\\kelida\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\Users\\kelida\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\kelida\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\kelida\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\kelida\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n    return self.function(*args, **kwargs)\n  File \"C:\\Users\\kelida\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 668, in _fit_and_score\n    estimator = estimator.set_params(**cloned_parameters)\n  File \"C:\\Users\\kelida\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\pipeline.py\", line 188, in set_params\n    self._set_params(\"steps\", **kwargs)\n  File \"C:\\Users\\kelida\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\metaestimators.py\", line 54, in _set_params\n    super().set_params(**params)\n  File \"C:\\Users\\kelida\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py\", line 245, in set_params\n    raise ValueError(\nValueError: Invalid parameter tfidf_use_idf for estimator Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n                ('clf', SGDClassifier())]). Check the list of available parameters with `estimator.get_params().keys()`.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8752/2231019290.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspam_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'message'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mspam_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'response'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Done in %0.3fs'\u001b[0m\u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    889\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1390\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1392\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    836\u001b[0m                     )\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m                 out = parallel(\n\u001b[0m\u001b[0;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[0;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    936\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    443\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m                 \u001b[1;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter tfidf_use_idf for estimator Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n                ('clf', SGDClassifier())]). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "#perform the grid search with pipeline and parameters\n",
    "grid_search = GridSearchCV(pipeline, parameters,n_jobs=-1,verbose=1)\n",
    "print('Performing grid search now...')\n",
    "print('Parameters: ')\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(spam_df['message'],spam_df['response'])\n",
    "print('Done in %0.3fs'% (time().t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515caba6",
   "metadata": {},
   "source": [
    "Grid search consumes a lot of memory and n_jobs=-1 instructs the process to use all cpus while processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bc34e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e906802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2277d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76381ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a06531d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2cfa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
