{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd28e5ec",
   "metadata": {},
   "source": [
    "# NLP  or Natural Language Processing\n",
    "an automated way to understand and analyze natural human language and extract informaion from such data by applying machine algorithms\n",
    "- ie extract the linguistic information \n",
    "- the content could be image, audio, video or text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb560c9",
   "metadata": {},
   "source": [
    "### Why NLP\n",
    "The world is now connected globally due to the advancement of technology and devices this has resulted in the high volume of digital data across the world and has led to a number of challenges in analyzing data including:\n",
    "- analyzing tones of data generated in the form of text, image, audios and videos\n",
    "- identifying various languages and dialects followed across the globe\n",
    "- applying quantitative analysis on huge collections of data\n",
    "- handling ambiquities while interpreting data and extracting information\n",
    "\n",
    "#### With this advancement in tech and services the world is now a global village and this is wherenlp proves useful\n",
    "\n",
    "##### One of the main goals of NLP :\n",
    "- Understand various languages, process them and extract information from them.\n",
    "\n",
    "\n",
    "In NLP full automation can be easily achieved by using modern software libraries, modules, and packages\n",
    "With the help of these libraries we can also build analytical models and automate the nlp process with minimum or no human interventions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac220b",
   "metadata": {},
   "source": [
    "### NLP Terminology\n",
    "###### 1. Word boundaries\n",
    "- determines where one word ends and another begins\n",
    "###### 2. Tokenization\n",
    "- a technique to split words, phrases, idioms etc,present in a doc\n",
    "###### 3. Stemming\n",
    "- a process to map wordsto thire stem or root. It is very useful in finding synonyms, and extensiviley used in search engines\n",
    "###### 4. Tf-idf\n",
    "- a numerical value which represents how important a word is to a document or corpus\n",
    "###### 5. Semantic analytics\n",
    "- a technique in vectorial semantics of analysing relationships between a set of docs and the terms it contains by producing a set of concepts related to the documents and terms\n",
    "i.e compares words phrases and idioms in a set of documents to extract meaning\n",
    "###### 6. Disambiguation\n",
    "- Technique to determine the meaning and a sense of words(context vs intent)\n",
    "###### 7. Topic models\n",
    "- type of statistical model that find abstract topics which occur in a collection of documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d011ef",
   "metadata": {},
   "source": [
    "### NLP Approach for Text Data\n",
    "This approaches can be interalated or independently applied depending on the type of data to analyze\n",
    "- basic text processing - extract key words\n",
    "- categorizing and tagging words - eg tag a word using chines\n",
    "- classify text - identify particular feature of language\n",
    "- extract information - identify relationships in a structured way\n",
    "- analyze sentence structure - capture formal gramma\n",
    "- build feature based structure - insight into text arguments\n",
    "- analyze the meaning - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d4af4b",
   "metadata": {},
   "source": [
    "##### Eliminate punctuations and stopwords from the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1a02265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required library\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ba3de2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view first 10 stopwords present in the english corpus\n",
    "stopwords.words('english')[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9cb78a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a test senetnce\n",
    "test = 'This is my first test string. Wow!! we are doing just fine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "995aed60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'm',\n",
       " 'y',\n",
       " ' ',\n",
       " 'f',\n",
       " 'i',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 't',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 's',\n",
       " 't',\n",
       " 'r',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'W',\n",
       " 'o',\n",
       " 'w',\n",
       " ' ',\n",
       " 'w',\n",
       " 'e',\n",
       " ' ',\n",
       " 'a',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'd',\n",
       " 'o',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 'j',\n",
       " 'u',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 'f',\n",
       " 'i',\n",
       " 'n',\n",
       " 'e']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# noweliminate the punctuations and print them as a whole sentence\n",
    "no_punctuation = [char for char in test if char not in string.punctuation]\n",
    "no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d4e1a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is my first test string Wow we are doing just fine'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_punctuation = ''.join(no_punctuation)\n",
    "no_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0dad540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'my',\n",
       " 'first',\n",
       " 'test',\n",
       " 'string',\n",
       " 'Wow',\n",
       " 'we',\n",
       " 'are',\n",
       " 'doing',\n",
       " 'just',\n",
       " 'fine']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split each word present in the new sentence\n",
    "no_punctuation.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c4df82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now eliminate stopwords\n",
    "clean_sentence = [word for word in no_punctuation.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2292e2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['first', 'test', 'string', 'Wow', 'fine']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the final cleaned sentence\n",
    "clean_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627e8558",
   "metadata": {},
   "source": [
    "### NLP Application\n",
    "#### 1. Machine Translation\n",
    "    - Machine translation is used to translate one language into another. Google translate is an example since it uses NLP to translate the input data from one lanuage to another\n",
    "    \n",
    "#### 2. Speech Recognition\n",
    "    - Speech recognition app understands human speech and uses it as input information. It is useful for apps eg Siri, Google Now and Microsoft Cortana\n",
    "\n",
    "#### 3. Sentiment Analysis\n",
    "    - Achieved by processing tons of data received from different interfaces and sources. eg nlp uses all social media activities to find out the popular topics of discussions or importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dafbc79",
   "metadata": {},
   "source": [
    "### Major NLP Libraries\n",
    " - NLTK\n",
    "\n",
    "Widely used in the industry to build programs to work with different human lanuages\n",
    " - Scikit-learn\n",
    "\n",
    "Powerful open source package designed for operating with other python libraries eg numpy and pandas\n",
    " - TextBlob\n",
    "\n",
    "Used for processing text data and provides a simple API'S for diving into NLP\n",
    " - spaCy\n",
    "\n",
    "A librarythat provides multiple useful views of textual meaning and lingusitic structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e194b7e1",
   "metadata": {},
   "source": [
    "### The Scikit-Learn Approach\n",
    "A very poerful library with set of modules to process and analyse natural language data such as texts, images and extract information using ML algorithms\n",
    " - Built in module to load the datasets, content and categories\n",
    " \n",
    " - Feature extraction, a way to extract information from data that can be text or images\n",
    " \n",
    " - Model training, analyze the content based on particular categories and then train them according to a specific model\n",
    " \n",
    " - Pipeline building mechanism, a technique in sklearn to streamline the NLP process into stages\n",
    "      -  Vectorization - coverting a collection of text of documents into a numerical feature vector\n",
    "      -  Transformation - extract features around the word of interest\n",
    "      -  Modeltraining and application - required for accurate predictions\n",
    " \n",
    " - Performance Optimization, we train the model to optimize the overall process\n",
    " \n",
    " - Grid search for finding good parameters, a powerfull way to search parameters affecting the outcome for model training purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca0897e",
   "metadata": {},
   "source": [
    "#### Modules to Load Conent and Categories\n",
    "sklearn has many built in datasets. There are several methods to load thes datasets with the help of a data load object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4d79350",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data set\n",
    "#load_data = sklearn.datasets.load_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f49029",
   "metadata": {},
   "source": [
    "Text files are loaded with categories as subfolder names\n",
    "\n",
    "The folder names are used as supervised signal label names\n",
    "\n",
    "It doesn't try to extract features into a numpy array or scipy sparse matrix\n",
    "\n",
    "If load conetnt parameter is false, it does not try to load the file in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0bd6d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a feature extraction transformer\n",
    "#from sklearn.feature_extraction.text import <appropriate transformer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54ad4fe",
   "metadata": {},
   "source": [
    "##### Attributesof a data load objects are :\n",
    " - Bunch, contains fields and can be accessed as dict keys or an object\n",
    " - Target names, list of requested categories\n",
    " - Data, attribute in the memory where files are loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc279909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "160fa3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create object of the load daa\n",
    "digit_data = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1994f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 1797\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use  built in descr fn to describe the data set\n",
    "digit_data.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdad456c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view type of data\n",
    "type(digit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "139a0fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "       [ 0.,  0., 10., ..., 12.,  1.,  0.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view data\n",
    "digit_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cea506c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 8])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view target\n",
    "digit_data.target\n",
    "#the arrays displayed are the response data which is present in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56271c04",
   "metadata": {},
   "source": [
    "#### Feature Extraction\n",
    "a method to convert the conent intonumerical vectors to perform ml\n",
    " 1. Text feature extraction eg large datasets or documents\n",
    " \n",
    " 2. Image feature extraction eg patch extraction/connectivity graph of an image and hierachical clusering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b275f1",
   "metadata": {},
   "source": [
    "#### Bag of Words\n",
    "used to convert text data int numerical feature vector witha fixed size\n",
    "- tokenize - count number of occurences of each word - store as the value feature"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b850e747",
   "metadata": {},
   "source": [
    "class\n",
    "sklearn.feature_extraction.text.CountVectorizer- specifies the number of components to keep\n",
    "(input = 'content',-file name of sequence of string\n",
    "encoding='utf-8', - encoding used to decode the input\n",
    "decode_error='strict',\n",
    "strip_accent=None,-removes accents\n",
    "lowercase=True,\n",
    "preprocessor=None,\n",
    "tokenizer=None,-overides string tokenizer\n",
    "stop_words=None, - built in stopwords list\n",
    "token_pattern='(?u)\\b\\w\\w+\\b',\n",
    "ngram_range=(1,1),\n",
    "analyzer='word',\n",
    "max_df=1.0, max threshhold which should be ignored\n",
    "min_df=1, - min threshhold which should be ignored\n",
    "max_features=None,\n",
    "vocubulary=None,\n",
    "binary=False,\n",
    "dtype=<class'numpy.int64'>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48bc2152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56936796",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimate the vectorizer\n",
    "vector = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bda5abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 3 documents\n",
    "doc1 = 'Hi How are you'\n",
    "doc2 = 'today is a very very very pleasant day we can have some fun fun fun'\n",
    "doc3 = 'this was an amazing experience'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70147ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#put them together\n",
    "docs = [doc1, doc2, doc3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf8563ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit them as bag of words\n",
    "bag_of_words = vector.fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f732c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d709987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply tranfrom method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aa6927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e724dce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb344c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e916b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9036d901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f05c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6205bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9789d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46cf794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2360a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fba866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76114d39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6b57d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a404343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb3b04e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c20ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bc34e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e906802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2277d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76381ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a06531d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b2cfa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
